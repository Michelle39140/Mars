{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/Wusi/Anaconda3/chromedriver\n"
     ]
    }
   ],
   "source": [
    "# https://splinter.readthedocs.io/en/latest/drivers/chrome.html\n",
    "!which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Splinter\n",
    "executable_path = {'executable_path': 'c://Users/Wusi/Anaconda3/chromedriver'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. NASA Mars News----------------------------------------\n",
    "## Scrape the NASA Mars News Site (https://mars.nasa.gov/news) and collect the latest News Title and Paragraph Text\n",
    "## Assign the text to variables to reference later\n",
    "\n",
    "#! can't use requests library here, because the news are rendered by js after page is load; if use requests.get, it will only return the contents before rendering\n",
    "\n",
    "# 1.1 Retrieve page with splinter\n",
    "url_news = \"https://mars.nasa.gov/news\"\n",
    "browser.visit(url_news)\n",
    "html = browser.html\n",
    "\n",
    "# 1.2 Get the first news from html retrieved \n",
    "# Create BeautifulSoup object; parse with 'html.parser'\n",
    "bsoup = bs(html, 'html.parser')\n",
    "\n",
    "# reach the container of the first news \n",
    "li = bsoup.find(\"li\",class_=\"slide\")\n",
    "\n",
    "news_t = li.find(\"div\",class_=\"content_title\").text  # title\n",
    "news_p = li.find(\"div\",class_=\"article_teaser_body\").text # paragraph\n",
    "news_link = url_news.replace(\"/news\",\"\")+li.find(\"div\",class_=\"content_title\").a[\"href\"] # link to the news (added to base url)\n",
    "news_date = li.find(\"div\",class_=\"list_date\").text # date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. JPL Mars Space Images - Featured Image----------------------------------\n",
    "## Get the current Featured Image from JPL (https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars)\n",
    "\n",
    "url_img = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "\n",
    "# navigate to to full-size image url with splinter\n",
    "browser.visit(url_img)\n",
    "browser.click_link_by_partial_text('FULL IMAGE')\n",
    "\n",
    "# ---if try to click on the more info button, directly, sometimes it returns an error \"element not visible\"\n",
    "# ---the only way to avoid that it to wait until the element becomes visible, which takes time\n",
    "# --- the workaround is to get the href link and visit it insteading of trying to click the link directly\n",
    "# time.sleep(30)\n",
    "# browser.click_link_by_partial_text('more info')\n",
    "\n",
    "href= browser.find_link_by_partial_text(\"more info\")[0][\"href\"]\n",
    "browser.visit(href)\n",
    "\n",
    "browser.find_by_css(\".main_image\").click()\n",
    "\n",
    "# store the image url\n",
    "featured_image_url = browser.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mars Weather -------------------------------------------------\n",
    "## Visit the Mars Weather twitter account page (https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page \n",
    "\n",
    "# 3.1 Retrieve page using requests\n",
    "url_twitter = \"https://twitter.com/marswxreport?lang=en\"\n",
    "html = requests.get(url_twitter).text\n",
    "\n",
    "# 3.2 Get the weather post from html retrieved\n",
    "bsoup = bs(html,\"html.parser\")\n",
    "\n",
    "# all tweets are under ol\n",
    "ol = bsoup.find(id=\"stream-items-id\")\n",
    "\n",
    "# put tweets in lis list\n",
    "lis = ol.findAll(\"li\")\n",
    "# use a for loop to find the first tweet with weather info (criterion: has hPa in the post)\n",
    "mars_weather = \"\"\n",
    "for li in lis:\n",
    "    tweet = li.find(\"div\",class_=\"js-tweet-text-container\").p.text\n",
    "    if tweet.find(\"hPa\"):\n",
    "        mars_weather = tweet\n",
    "        break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Mars Facts------------------------------------------------------\n",
    "## Visit the Mars Facts webpage (https://space-facts.com/mars/) and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc.\n",
    "# Use Pandas to convert the data to a HTML table string.\n",
    "url_fact = \"https://space-facts.com/mars/\"\n",
    "\n",
    "# use pandas to scrape tabular data from the page\n",
    "tables = pd.read_html(url_fact)\n",
    "facts = tables[0]\n",
    "\n",
    "# store data in a list of lists\n",
    "facts = facts.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Mars Hemispheres-------------------------------------------------------------\n",
    "## Visit the USGS Astrogeology site (https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres.\n",
    "\n",
    "# 5.1 Retrieve the html with splinter\n",
    "url_hemi = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "browser.visit(url_hemi)\n",
    "html = browser.html\n",
    "\n",
    "# 5.2 Get the urls needed from the html retrieved\n",
    "bsoup = bs(html,\"html.parser\")\n",
    "\n",
    "items = bsoup.findAll(\"div\",class_=\"item\")\n",
    "\n",
    "hemisphere_image_urls=[] # initialize list\n",
    "for item in items:\n",
    "    title = item.find(\"h3\").text # title\n",
    "    url = \"https://astrogeology.usgs.gov/\"+item.find(\"div\",class_=\"description\").a[\"href\"] # get the url for picture details \n",
    "    browser.visit(url)\n",
    "    img_url = browser.find_link_by_text(\"Sample\")[0][\"href\"] # get the url to the full-size picture\n",
    "    hemisphere_image_urls.append( {\"title\":title, \"img_url\":img_url} ) # append a dictionary to the hemisphere_image_urls list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store data scraped into a dictionary--------------------------------------------------------------------\n",
    "data = {\n",
    "    \"news\":{\n",
    "        \"title\":news_t,\n",
    "        \"body\":news_p,\n",
    "        \"link\":news_link,\n",
    "        \"date\":news_date\n",
    "    },\n",
    "    \"feature_img\":featured_image_url,\n",
    "    \"weather\":mars_weather,\n",
    "    \"facts\":facts,\n",
    "    \"hemi_img\":hemisphere_image_urls \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
